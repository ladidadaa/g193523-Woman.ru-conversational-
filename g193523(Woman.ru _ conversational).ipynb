{"cells":[{"cell_type":"code","execution_count":null,"id":"66a8f948-ec86-47b1-882c-11396f215f61","metadata":{"id":"66a8f948-ec86-47b1-882c-11396f215f61"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import json\n","import os\n","import requests"]},{"cell_type":"markdown","source":["# –ü–∞—Ä—Å–∏–Ω–≥"],"metadata":{"id":"vnJUiSaWj77p"},"id":"vnJUiSaWj77p"},{"cell_type":"code","execution_count":null,"id":"ea56e84c","metadata":{"id":"ea56e84c"},"outputs":[],"source":["#@title –°–±–æ—Ä –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ç–µ–º–∞–º–∏ —Ñ–æ—Ä—É–º–∞\n","forums_page_list = []\n","response = requests.get('https://www.woman.ru/forum/?sort=30d')\n","count = 2\n","while response.status_code != 404:\n","    forums_page_list.append(response.text)\n","    response = requests.get(f'https://www.woman.ru/forum/{count}/?sort=30d')\n","    count += 1"]},{"cell_type":"code","execution_count":null,"id":"8900776e","metadata":{"id":"8900776e"},"outputs":[],"source":["#@title –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–µ–º—ã\n","href_list = []\n","for page in forums_page_list:\n","    soup = BeautifulSoup(page)\n","    links = soup.find_all(class_='list-item__link')\n","    for link in links:\n","        href_list.append(link['href'])\n"]},{"cell_type":"code","execution_count":null,"id":"6bb6cd31-3b1e-4601-9d87-a811d7ef0548","metadata":{"id":"6bb6cd31-3b1e-4601-9d87-a811d7ef0548"},"outputs":[],"source":["#@title –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–º–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π\n","comments_list = []\n","for link in href_list:  # –ò–¥–µ–º –ø–æ –≤—Å–µ–º —Ç–µ–º–∞–º\n","    params = {}\n","    r = requests.post(f\"https://www.woman.ru{href_list[0]}\", params)\n","    temp_page_comments_list = []  # –°–ø–∏—Å–æ–∫ —Å –±–ª–æ–∫–∞–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤\n","    response = requests.get(f'https://www.woman.ru{link}', cookies=r.cookies)\n","    count = 2\n","    while response.status_code != 404:\n","        temp_page_comments_list.append(response.text)\n","        response = requests.get(f'https://www.woman.ru{link}{count}', cookies=r.cookies)\n","        count += 1\n","    for temp_page in temp_page_comments_list: # –ò–¥–µ–º –ø–æ –≤—Å–µ–º –∫–æ–º–º–µ–Ω—Ç–∞–º –∏ –∑–∞–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏–µ - –æ—Ç–≤–µ—Ç\n","        soup = BeautifulSoup(temp_page)\n","        comments_block = soup.find_all(class_='card__text')\n","        for comment in comments_block:\n","            if comment.find_all(class_='quote'):\n","                comments_list.append((comment.find_all(class_='quote__text')[0].text, comment.find_all(class_='card__comment')[0].text))\n","    # with open('cache.json', 'w') as f:\n","    #     json.dump(comments_list, f)            "]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJ5lS3IZZO5_","executionInfo":{"status":"ok","timestamp":1640784811262,"user_tz":-180,"elapsed":17094,"user":{"displayName":"sss SHAHT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkPMF1ZeILVDhLjcLPi2vZ8kdNgs_ZQIN_cIjh=s64","userId":"09644874298111193672"}},"outputId":"189faa71-2f44-4cf8-a71a-e20f358f17ec"},"id":"MJ5lS3IZZO5_","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["with open('cache.json', 'w', encoding='utf-8') as f:\n","    json.dump(comments_list, f, indent=4, ensure_ascii=False)"],"metadata":{"id":"8Eh0Gf6nYRBT"},"id":"8Eh0Gf6nYRBT","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"a4d1b538","metadata":{"id":"a4d1b538"},"outputs":[],"source":["with open('dataset.json', 'r', encoding='utf-8') as f:\n","    forum_data = json.load(f)\n","    "]},{"cell_type":"markdown","source":["### –ü–æ–ª—É—á–∏–≤—à–∏–µ—Å—è –¥–∞–Ω–Ω—ã–µ"],"metadata":{"id":"d6pGTxL1kg9e"},"id":"d6pGTxL1kg9e"},{"cell_type":"code","source":["from pandas.io.json import json_normalize"],"metadata":{"id":"GjPbrHu5YbiA"},"id":"GjPbrHu5YbiA","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"aff7f4f5","metadata":{"id":"aff7f4f5"},"outputs":[],"source":["formated_data = []\n","for item in forum_data:\n","    formated_data.append({'comment': item[0], 'answer': item[1]})"]},{"cell_type":"code","source":["with open('cache_formated.json', 'w', encoding='utf-8') as f:\n","    json.dump(formated_data, f, indent=4, ensure_ascii=False)"],"metadata":{"id":"0KwrMru-fwEn"},"id":"0KwrMru-fwEn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('cache_formated.json', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","    text = json.loads(text)"],"metadata":{"id":"2ywIX5lSgDvC"},"id":"2ywIX5lSgDvC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["json_normalize(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":839},"id":"dYkXx8ttgYLc","executionInfo":{"status":"ok","timestamp":1640787394608,"user_tz":-180,"elapsed":987,"user":{"displayName":"sss SHAHT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkPMF1ZeILVDhLjcLPi2vZ8kdNgs_ZQIN_cIjh=s64","userId":"09644874298111193672"}},"outputId":"0664a825-b290-46c6-858e-67a2c6285d5d"},"id":"dYkXx8ttgYLc","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-2531ff66-536a-423e-a616-66d1ec65b932\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>–µ–º—É–ò –∂–≤–∞—á–∫–∞ –µ—â—ë üòÅ</td>\n","      <td>–ê–≥–∞–º))—ç—Ç–æ —è —Ç —Å–æ–∑–¥–∞–ª–∞,–Ω–µ –¥–ª—è —Ç—Ä–æ–ª–ª–µ–π,–∞ —á—Ç–æ–± —Ç—É...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>–ê–≥–∞–º))—ç—Ç–æ —è —Ç–µ–º—É —Å–æ–∑–¥–∞–ª–∞,–Ω–µ –¥–ª—è —Ç—Ä–æ–ª–ª–µ–π,–∞ —á—Ç–æ–±...</td>\n","      <td>–ü—Ä–∏–≤–µ—Ç. –Ø –ø–æ—ç—Ç–æ–º—É –∏ –ø—Ä–∏—à–ª–∞)) üòÅ —è –∂ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>–ü—Ä–∏–≤–µ—Ç. –Ø –ø–æ—ç—Ç–æ–º—É –∏ –ø—Ä–∏—à–ª–∞)) üòÅ —è –∂ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è ...</td>\n","      <td>–ü—Ä—è–º –∫–∞–∫ –î–∂–∏–≥—É—Ä–¥–∞ –≤ –æ–¥–Ω–æ–º —Ñ–∏–ª—å–º–µ –∫—Ä–∏—á–∞–ª - \"–Ø –Ω...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>–ü—Ä—è–º –∫–∞–∫ –î–∂–∏–≥—É—Ä–¥–∞ –≤ –æ–¥–Ω–æ–º —Ñ–∏–ª—å–º–µ –∫—Ä–∏—á–∞–ª - \"–Ø –Ω...</td>\n","      <td>–¢–∏–ø–∞ —Ç–æ–≥–æ üòÅüòÅüòÅüòÅ –±–ª–∏–Ω —è –∞–∂ –∏–Ω—Ç–æ–Ω–∞—Ü–∏—é –µ–≥–æ –≥–æ–ª–æ—Å–∞ ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>–¢–∏–ø–∞ —Ç–æ–≥–æ üòÅüòÅüòÅüòÅ –±–ª–∏–Ω —è –∞–∂ –∏–Ω—Ç–æ–Ω–∞—Ü–∏—é –µ–≥–æ –≥–æ–ª–æ—Å–∞ ...</td>\n","      <td>üòÇüòÇüòÇüòÇ–∏ —è))</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>126102</th>\n","      <td>1. –ù–µ —Ç–æ —á—Ç–æ–±—ã —Ä–∞–∑–±–∏–ª–∞. –°–∫–æ—Ä–µ–µ —è –±—ã–ª —à–æ–∫–∏—Ä–æ–≤–∞–Ω...</td>\n","      <td>3. –î–∞ –±–æ–∂–µ –∂ —Ç—ã –º–æ–π!!! –ù–∞ –∫–æ–º –Ω–µ –∂–µ–Ω—è—Ç—Å—è??? –í–µ...</td>\n","    </tr>\n","    <tr>\n","      <th>126103</th>\n","      <td>1. ))) –ù–∏–∫—Ç–æ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ç—Ä–æ–∏—Ç. –í—ã –ª–∏–±–æ –ø–æ–¥—Ö–æ–¥–∏...</td>\n","      <td>1. –í–µ, –∫–∞–∫ –∂–µ —Ç—ã —É–∂–µ –¥–æ—Å—Ç–∞–ª! –°—Ç–æ–∏–ª–æ –∂–µ–Ω–∏—Ç—å—Å—è –Ω...</td>\n","    </tr>\n","    <tr>\n","      <th>126104</th>\n","      <td>1. –ù—É —è –∂–µ –Ω–µ –≤–∑—è–ª –µ—ë —á–µ—Å—Ç—å –≤ 16 –ª–µ—Ç –∫–∞–∫ –æ–Ω–∞ —Ö...</td>\n","      <td>1. –ù–µ—Ç, —Ç—ã –æ—Ç–Ω—é–¥—å –Ω–µ —Ä–∞–∑—É–º–Ω–µ–µ –µ—ë. –•–æ—Ç—è –±—ã –ø–æ—Ç–æ...</td>\n","    </tr>\n","    <tr>\n","      <th>126105</th>\n","      <td>–Ø –ø—Ä–∏–ª–∏—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ–ª–æ–≤–µ–∫ –≤—Å—Ç—Ä–µ—á–∞–ª–∞ —Å —Ç–∞–∫...</td>\n","      <td>–í —É—Ä–∞–ª—å—Å–∫–æ–π –≥–ª—É–±–∏–Ω–∫–µ –í–µ –∂–∏–≤—ë—Ç. –î–µ—Ä–µ–≤–Ω—è –ú—É—Ä–∑–∏–Ω–∫...</td>\n","    </tr>\n","    <tr>\n","      <th>126106</th>\n","      <td>*** ü•¥–Ø –Ω–∏–∫–∞–∫ –Ω–µ –º–æ–≥—É —ç—Ç–æ –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å ) –•...</td>\n","      <td>–í–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–µ—Å–∫–∏ –ø—Ä–æ—Ç–∏–≤ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫. –¢–æ–ª—å–∫–æ ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>126107 rows √ó 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2531ff66-536a-423e-a616-66d1ec65b932')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2531ff66-536a-423e-a616-66d1ec65b932 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2531ff66-536a-423e-a616-66d1ec65b932');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                  comment                                             answer\n","0                                       –µ–º—É–ò –∂–≤–∞—á–∫–∞ –µ—â—ë üòÅ  –ê–≥–∞–º))—ç—Ç–æ —è —Ç —Å–æ–∑–¥–∞–ª–∞,–Ω–µ –¥–ª—è —Ç—Ä–æ–ª–ª–µ–π,–∞ —á—Ç–æ–± —Ç—É...\n","1       –ê–≥–∞–º))—ç—Ç–æ —è —Ç–µ–º—É —Å–æ–∑–¥–∞–ª–∞,–Ω–µ –¥–ª—è —Ç—Ä–æ–ª–ª–µ–π,–∞ —á—Ç–æ–±...  –ü—Ä–∏–≤–µ—Ç. –Ø –ø–æ—ç—Ç–æ–º—É –∏ –ø—Ä–∏—à–ª–∞)) üòÅ —è –∂ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è ...\n","2       –ü—Ä–∏–≤–µ—Ç. –Ø –ø–æ—ç—Ç–æ–º—É –∏ –ø—Ä–∏—à–ª–∞)) üòÅ —è –∂ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è ...  –ü—Ä—è–º –∫–∞–∫ –î–∂–∏–≥—É—Ä–¥–∞ –≤ –æ–¥–Ω–æ–º —Ñ–∏–ª—å–º–µ –∫—Ä–∏—á–∞–ª - \"–Ø –Ω...\n","3       –ü—Ä—è–º –∫–∞–∫ –î–∂–∏–≥—É—Ä–¥–∞ –≤ –æ–¥–Ω–æ–º —Ñ–∏–ª—å–º–µ –∫—Ä–∏—á–∞–ª - \"–Ø –Ω...  –¢–∏–ø–∞ —Ç–æ–≥–æ üòÅüòÅüòÅüòÅ –±–ª–∏–Ω —è –∞–∂ –∏–Ω—Ç–æ–Ω–∞—Ü–∏—é –µ–≥–æ –≥–æ–ª–æ—Å–∞ ...\n","4       –¢–∏–ø–∞ —Ç–æ–≥–æ üòÅüòÅüòÅüòÅ –±–ª–∏–Ω —è –∞–∂ –∏–Ω—Ç–æ–Ω–∞—Ü–∏—é –µ–≥–æ –≥–æ–ª–æ—Å–∞ ...                                          üòÇüòÇüòÇüòÇ–∏ —è))\n","...                                                   ...                                                ...\n","126102  1. –ù–µ —Ç–æ —á—Ç–æ–±—ã —Ä–∞–∑–±–∏–ª–∞. –°–∫–æ—Ä–µ–µ —è –±—ã–ª —à–æ–∫–∏—Ä–æ–≤–∞–Ω...  3. –î–∞ –±–æ–∂–µ –∂ —Ç—ã –º–æ–π!!! –ù–∞ –∫–æ–º –Ω–µ –∂–µ–Ω—è—Ç—Å—è??? –í–µ...\n","126103  1. ))) –ù–∏–∫—Ç–æ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ç—Ä–æ–∏—Ç. –í—ã –ª–∏–±–æ –ø–æ–¥—Ö–æ–¥–∏...  1. –í–µ, –∫–∞–∫ –∂–µ —Ç—ã —É–∂–µ –¥–æ—Å—Ç–∞–ª! –°—Ç–æ–∏–ª–æ –∂–µ–Ω–∏—Ç—å—Å—è –Ω...\n","126104  1. –ù—É —è –∂–µ –Ω–µ –≤–∑—è–ª –µ—ë —á–µ—Å—Ç—å –≤ 16 –ª–µ—Ç –∫–∞–∫ –æ–Ω–∞ —Ö...  1. –ù–µ—Ç, —Ç—ã –æ—Ç–Ω—é–¥—å –Ω–µ —Ä–∞–∑—É–º–Ω–µ–µ –µ—ë. –•–æ—Ç—è –±—ã –ø–æ—Ç–æ...\n","126105  –Ø –ø—Ä–∏–ª–∏—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ–ª–æ–≤–µ–∫ –≤—Å—Ç—Ä–µ—á–∞–ª–∞ —Å —Ç–∞–∫...  –í —É—Ä–∞–ª—å—Å–∫–æ–π –≥–ª—É–±–∏–Ω–∫–µ –í–µ –∂–∏–≤—ë—Ç. –î–µ—Ä–µ–≤–Ω—è –ú—É—Ä–∑–∏–Ω–∫...\n","126106  *** ü•¥–Ø –Ω–∏–∫–∞–∫ –Ω–µ –º–æ–≥—É —ç—Ç–æ –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å ) –•...  –í–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—á–µ—Å–∫–∏ –ø—Ä–æ—Ç–∏–≤ –æ—Ä–∞–ª—å–Ω—ã—Ö –ª–∞—Å–∫. –¢–æ–ª—å–∫–æ ...\n","\n","[126107 rows x 2 columns]"]},"metadata":{},"execution_count":34},{"output_type":"stream","name":"stdout","text":["Warning: total number of rows (126107) exceeds max_rows (20000). Limiting to first (20000) rows.\n"]}]},{"cell_type":"markdown","source":["# –ù–µ–π—Ä–æ–Ω–∫–∞"],"metadata":{"id":"bCp1ahfQkGTX"},"id":"bCp1ahfQkGTX"},{"cell_type":"code","source":["! pip install datasets transformers[sentencepiece]\n","! pip install accelerate"],"metadata":{"id":"7JxAcGYNkO2C"},"id":"7JxAcGYNkO2C","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ce65b2ab","metadata":{"id":"ce65b2ab"},"outputs":[],"source":["import sys\n","import re\n","import json\n","\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","import torch\n","from transformers import TextDataset, DataCollatorForLanguageModeling\n","from torch.utils.data import DataLoader\n","\n","from accelerate import Accelerator\n","from transformers import AdamW, AutoModelForSequenceClassification, get_cosine_schedule_with_warmup"]},{"cell_type":"code","execution_count":null,"id":"379528bc","metadata":{"id":"379528bc"},"outputs":[],"source":["#@title –ß–µ–∫–ø–æ–∏–Ω—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å hugging face\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","checkpoint = \"Kirili4ik/ruDialoGpt3-medium-finetuned-telegram\"   \n","tokenizer =  AutoTokenizer.from_pretrained(checkpoint)\n","model = AutoModelForCausalLM.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"id":"e41a4716","metadata":{"id":"e41a4716"},"outputs":[],"source":["#@title –ì–æ—Ç–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã\n","\n","\n","def get_length_param(text: str, tokenizer) -> str:\n","    \"\"\"Maps text to 1 of 4 buckets based on length after encoding.\n","\n","    Parameters\n","    ----------\n","    text: str\n","        The text to be given 1 of 4 length parameters.\n","\n","    tokenizer: HuggingFace tokenizer \n","        Tokenizer that used to compute the length of the text after encoding.\n","        For more info ee https://huggingface.co/transformers/main_classes/tokenizer.html\n","\n","    Returns\n","    -------\n","    len_param: str\n","        One of four buckets: \n","        '1' for short, '2' for medium, '3' for long texts and '-' for all others. \n","    \"\"\"\n","    tokens_count = len(tokenizer.encode(text))\n","    if tokens_count <= 15:\n","        len_param = '1'\n","    elif tokens_count <= 50:\n","        len_param = '2'\n","    elif tokens_count <= 256:\n","        len_param = '3'\n","    else:\n","        len_param = '-'\n","    return len_param\n","\n","\n","def get_user_param(text: dict, machine_name_in_chat: str) -> str:\n","    \"\"\"Maps text by 1/0 for it to be the person or the machine in the dialogue\n","\n","    Parameters\n","    ----------\n","    text: Dict[..., 'from', ...]\n","        Dict containing field 'from' with the name of the user who sent the message\n","\n","    machine_name_in_chat: str\n","        Str with the name of the machine - it will be predicted\n","    \"\"\"\n","    if text['from'] == machine_name_in_chat:\n","        return '1'  # machine\n","    else:\n","        return '0'  # human\n","\n","# –ó–¥–µ—Å—å —É–∫–∞–∑—ã–≤–∞–µ–º –∫–ª—é—á –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\n","def build_text_file(data_json: dict, dest_path: str, \n","                    tokenizer, machine_name_in_chat='first'):\n","    \"\"\"Create a text file for training in special format for ruDialoGPT-3.\n","\n","    Parameters\n","    ----------\n","    data_json: dict\n","        Dict containing 'text' (message) and 'from' (user who sent the message)\n","        \n","    dest_path: str\n","        String containing path to write data there\n","\n","    tokenizer: HuggingFace tokenizer \n","        Tokenizer that used to compute the length of the text after encoding.\n","        For more info ee https://huggingface.co/transformers/main_classes/tokenizer.html\n","    \"\"\"\n","    f = open(dest_path, 'w', encoding='utf-8')\n","    new_data = ''\n","    for i in range(len(data_json) - 1):\n","        message, next_message = data_json[i], data_json[i+1]\n","        if message['text'] == '' or type(message['text']) != str:\n","            continue\n","        if next_message['text'] == '' or type(next_message['text']) != str:\n","            continue\n","\n","        user   = get_user_param(message, machine_name_in_chat=machine_name_in_chat)\n","        length = get_length_param(data_json[i+1]['text'], tokenizer)\n","        message_text = re.sub(r\"\\n\", \". \", message['text'])\n","        new_data += f\"|{user}|{length}|{message_text}{tokenizer.eos_token}\" + \"\\n\"\n","\n","    f.write(new_data.encode('utf-8').decode('utf-8'))\n","\n","\n","def load_dataset(train_path, test_path, tokenizer):\n","    \"\"\"Creates train and test PyTorch datasets and collate_fn using HuggingFace.\n","\n","    Parameters\n","    ----------\n","    train_path: str\n","        String containing path to train data\n","        \n","    test_path: str\n","        String containing path to test data\n","\n","    tokenizer: HuggingFace tokenizer \n","        Tokenizer that used to compute the length of the text after encoding.\n","        For more info ee https://huggingface.co/transformers/main_classes/tokenizer.html\n","    \"\"\"\n","    train_dataset = TextDataset(\n","          tokenizer  = tokenizer,\n","          file_path  = train_path,\n","          block_size = 256)\n","     \n","    test_dataset = TextDataset(\n","          tokenizer  = tokenizer,\n","          file_path  = test_path,\n","          block_size = 256)   \n","    \n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer=tokenizer, mlm=False\n","    )\n","    return train_dataset, test_dataset, data_collator"]},{"cell_type":"code","source":["with open('dataset.json', 'r', encoding='utf-8') as f:\n","    forum_data = json.load(f)"],"metadata":{"id":"t0V10Wz1mgcB"},"id":"t0V10Wz1mgcB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title –ü–æ–¥–≥–æ–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç\n","formated_data = []\n","for item in forum_data:\n","    formated_data.append({'from': 'first', 'text': item[0]})\n","    formated_data.append({'from': 'second', 'text': item[1]})"],"metadata":{"id":"IioDQfDDiuhv"},"id":"IioDQfDDiuhv","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"f57d474f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f57d474f","executionInfo":{"status":"ok","timestamp":1640788973867,"user_tz":-180,"elapsed":65661,"user":{"displayName":"sss SHAHT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkPMF1ZeILVDhLjcLPi2vZ8kdNgs_ZQIN_cIjh=s64","userId":"09644874298111193672"}},"outputId":"b0f8632e-e409-4e40-eed9-197b44a1ac6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset length: 226993samples\n","Test dataset length: 25221samples\n"]}],"source":["#@title –°–±–æ—Ä–∫–∞ —Ñ–∞–π–ª–æ–≤\n","machine_name_in_chat = 'first' #@param {type : \"string\"}\n","\n","data = formated_data\n","\n","# test data is first 10% of chat, train - last 90%\n","train, test = data[int(len(data)*0.1):], data[:int(len(data)*0.1)]\n","\n","build_text_file(train, 'train_dataset.txt', tokenizer)\n","build_text_file(test,  'test_dataset.txt', tokenizer)\n","\n","print(\"Train dataset length: \" + str(len(train)) + \"samples\")\n","print(\"Test dataset length: \"  + str(len(test)) + \"samples\")"]},{"cell_type":"code","execution_count":null,"id":"fedcbdd0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fedcbdd0","executionInfo":{"status":"ok","timestamp":1640788744620,"user_tz":-180,"elapsed":280,"user":{"displayName":"sss SHAHT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkPMF1ZeILVDhLjcLPi2vZ8kdNgs_ZQIN_cIjh=s64","userId":"09644874298111193672"}},"outputId":"232b3815-1b44-4acd-b0cc-8d9491f994b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["|0|1|–ú–∏–ª—ã–π –Ω–∞ —ç—Ç–∏—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –æ—á–µ–Ω—å –æ—Ç–ª–∏—á–∏–ª—Å—è. –û—á–µ–Ω—å. –ò –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å —ç—Ç–∏–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–æ—Ç —ç—Ç–∏—Ö –º–æ–∑–≥–æ–≤–∑—Ä—ã–≤–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤. –Ø –≤ –∑–∞–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–µ –±—É–∫–≤–∞–ª—å–Ω–æ. –ù–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é —á—Ç–æ –¥–∞–ª—å—à–µ –±—É–¥–µ—Ç.</s>\n","|1|2|–ù—É –≤—Å–µ,–ê–Ω—å,—ç—Ç–æ –ø–ø–∏–ø–µ—Ü –ø–æ–ª–Ω—ã–π —Ç–æ–≥–¥–∞))</s>\n","|0|2|–°–æ–Ω—å—á–∏–∫, —ç—Ç–æ –≤–æ–æ–±—â–µ. –í —Ñ–µ–≤—Ä–∞–ª–µ –≥–æ–¥ –Ω–∞—à–µ–≥–æ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞. –ò —Ç–æ–ª—å–∫–æ –≤—á–µ—Ä–∞ —è —Å—Ç–∞–ª–∞ \"–ø–æ—Å—Ç–æ—è–Ω–Ω–∞—è —Ç—ë–ª–∞\"))</s>\n","|1|2|–Ø —Ç–∞–∫ –Ω–µ –¥—É–º–∞—é. –≠—Ç–æ –Ω–µ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–∏ —á—É–≤—Å—Ç–≤–∞ –¥–æ–ª–∂–Ω—ã –≤—ã–∫–ª—é—á–∏—Ç—å—Å—è. –ì–æ—Å–ø–æ–¥–∏, —É –Ω–µ–≥–æ –¥–∞–∂–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –ª–∏—Ü–∞, –∫–∞–∫ —É –æ—Ç—Ü–∞, —É–∂–∞—Åü§Øüò±</s>\n","|0|2|–ù–µ –ø—Ä–∏–¥–∞–≤–∞–π —ç—Ç–æ–º—É –∑–Ω–∞—á–µ–Ω–∏–µ, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ. –ù—É –ø–æ—Ö–æ–∂ –∏ –ø–æ—Ö–æ–∂.–ì–ª–∞–≤–Ω–æ–µ, –Ω–µ –¥—É–º–∞–π, —á—Ç–æ —ç—Ç–æ –∫–∞–∫–∞—è-—Ç–æ –∫–∞—Ä–º–∞, –∞ –ø—Ä–æ—Å—Ç–æ –æ—Å–æ–∑–Ω–∞–≤–∞–π, —á—Ç–æ —ç—Ç–æ –∑–∞–ª–æ–∂–µ–Ω–æ –æ—Ç –ø—Ä–∏—Ä–æ–¥—ã. –í—Å—ë –≤ –ø–æ—Ä—è–¥–∫–µ üòä</s>\n","|1|2|–ú–∏–ª—ã–π –Ω–∞ —ç—Ç–∏—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö –æ—á–µ–Ω—å –æ—Ç–ª–∏—á–∏–ª—Å—è. –û—á–µ–Ω—å. –ò –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å —ç—Ç–∏–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤–æ—Ç —ç—Ç–∏—Ö –º–æ–∑–≥–æ–≤–∑—Ä—ã–≤–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤. –Ø –≤ –∑–∞–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–µ –±—É–∫–≤–∞–ª—å–Ω–æ. –ù–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é —á—Ç–æ –¥–∞–ª—å—à–µ –±—É–¥–µ—Ç.</s>\n","|0|2|–ú–æ–∂–µ—Ç,–µ–º—É –ø—Ä–æ—Å—Ç–æ –Ω—É–∂–Ω–æ –±—ã–ª–æ –≤—ã–ø–ª–µ—Å–Ω—É—Ç—å?–ò –≤—Å—ë —Ç–∞–∫–∏ –æ–Ω –Ω–µ —Ç–∞–∫–æ–π –ø–ª–æ—Ö–æ–π?)–ù–µ –∑–Ω–∞—é –ê–Ω—å,—Ä–µ—à–∞—Ç—å —Ç–µ–±–µ,–∞ —Ç–æ –º—ã —Ç—É—Ç –Ω–∞—Å–æ–≤–µ—Ç—É–µ–º))</s>\n","|1|2|–°–æ–Ω—å—á–∏–∫, —ç—Ç–æ –≤–æ–æ–±—â–µ. –í —Ñ–µ–≤—Ä–∞–ª–µ –≥–æ–¥ –Ω–∞—à–µ–≥–æ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–∞. –ò —Ç–æ–ª—å–∫–æ –≤—á–µ—Ä–∞ —è —Å—Ç–∞–ª–∞ \"–ø–æ—Å—Ç–æ—è–Ω–Ω–∞—è —Ç—ë–ª–∞\"))</s>\n","|0|2|–¢–æ–≥–¥–∞,–ø–æ–ª—É—á–∞–π –ø–æ –º–∞–∫—Å–∏–º—É–º—É –æ—Ç —ç—Ç–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π.–ñ–∏–≤–∏ –æ–¥–Ω–∏–º –¥–Ω–µ–º.–ó–¥–µ—Å—å,—Å–µ–π—á–∞—Å,–≤ —ç—Ç—É –º–∏–Ω—É—Ç—É.–ù–µ –Ω–∞ —Å—Ç–æ–ª—å–∫–æ –∂–µ –≤—Å—ë –ø–ª–æ—Ö–æ...–ò–¥–µ–∞–ª—å–Ω—ã—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π-–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç,–∏–±–æ —ç—Ç–æ —Å–∫—É—á–Ω–æü§£</s>\n","|1|2|–ù–µ –ø—Ä–∏–¥–∞–≤–∞–π —ç—Ç–æ–º—É –∑–Ω–∞—á–µ–Ω–∏–µ, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ. –ù—É –ø–æ—Ö–æ–∂ –∏ –ø–æ—Ö–æ–∂.–ì–ª–∞–≤–Ω–æ–µ, –Ω–µ –¥—É–º–∞–π, —á—Ç–æ —ç—Ç–æ –∫–∞–∫–∞—è-—Ç–æ –∫–∞—Ä–º–∞, –∞ –ø—Ä–æ—Å—Ç–æ –æ—Å–æ–∑–Ω–∞–≤–∞–π, —á—Ç–æ —ç—Ç–æ –∑–∞–ª–æ–∂–µ–Ω–æ –æ—Ç –ø—Ä–∏—Ä–æ–¥—ã. –í—Å—ë –≤ –ø–æ—Ä—è–¥–∫–µ üòä</s>\n"]}],"source":["# let's look at our data\n","! head -n 10 train_dataset.txt"]},{"cell_type":"code","execution_count":null,"id":"074b90e6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"074b90e6","executionInfo":{"status":"ok","timestamp":1640789073989,"user_tz":-180,"elapsed":97270,"user":{"displayName":"sss SHAHT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkPMF1ZeILVDhLjcLPi2vZ8kdNgs_ZQIN_cIjh=s64","userId":"09644874298111193672"}},"outputId":"f6b424dd-c3d8-444c-ff6a-dcbc79da33b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"]}],"source":["# Create PyTorch Datasets\n","train_dataset, test_dataset, data_collator = load_dataset('train_dataset.txt', 'test_dataset.txt', tokenizer)\n","\n","# Create PyTorch Dataloaders\n","train_loader = DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=data_collator)\n","test_loader = DataLoader(test_dataset, batch_size=3, collate_fn=data_collator)"]},{"cell_type":"code","execution_count":null,"id":"fd072541","metadata":{"id":"fd072541"},"outputs":[],"source":["# this cell checks 1 forward pass\n","try:\n","    for batch in train_loader:\n","        break\n","    {k: v.shape for k, v in batch.items()}\n","\n","    outputs = model(**batch)\n","except:\n","    print(\"Unexpected error:\", sys.exc_info()[0])\n","    raise"]},{"cell_type":"code","execution_count":null,"id":"56893265","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":797},"id":"56893265","executionInfo":{"status":"error","timestamp":1640789177988,"user_tz":-180,"elapsed":87007,"user":{"displayName":"sss SHAHT","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkPMF1ZeILVDhLjcLPi2vZ8kdNgs_ZQIN_cIjh=s64","userId":"09644874298111193672"}},"outputId":"f25c760d-1d6d-4dd9-b4ba-fc58ce5b9a7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Who's phrase?\tH\n","===> Human: –ü—Ä–∏–≤–µ—Ç\n","Who's phrase?\tG\n","Phrase len? 1/2/3/-\t1\n","===> GPT-3:  –ù—É –≤–æ—Ç —è –Ω–µ –∑–Ω–∞—é\n","Who's phrase?\tH\n","===> Human: –ö–∞–∫–æ–π —Ç—ã –ª—é–±–∏—à—å —á–∞–π?\n","Who's phrase?\tG\n","Phrase len? 1/2/3/-\t3\n","===> GPT-3:  –ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è —á—ë—Ä–Ω—ã–π\n","Who's phrase?\tH\n","===> Human: –ê –µ—â–µ?\n","Who's phrase?\tG\n","Phrase len? 1/2/3/-\t2\n","===> GPT-3:  –ù—É —è –Ω–µ –æ—Å–æ–±–æ –ª—é–±–∏—Ç–µ–ª—å, –Ω–æ —Ç–∏–ø–∞ –≤–æ—Ç —É –º–µ–Ω—è –¥–æ–º–∞ –µ—Å—Ç—å –∏ —á–µ—Ä–Ω—ã–π –∏ –∑–µ–ª–µ–Ω—ã–π –∏ —Ç–¥\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-e0f2dd319520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnext_who\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Who's phrase?\\t\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#input(\"H / G?\")     # Human or GPT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# In case Human\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#@title –†–∞–±–æ—Ç–∞ —Å–µ—Ç–∏\n","chat_history_ids = torch.zeros((1, 0), dtype=torch.int)\n","\n","while True:\n","    \n","    next_who = input(\"Who's phrase?\\t\")  #input(\"H / G?\")     # Human or GPT\n","\n","    # In case Human\n","    if next_who == \"H\":\n","        input_user = input(\"===> Human: \")\n","        \n","        # encode the new user input, add parameters and return a tensor in Pytorch\n","        new_user_input_ids = tokenizer.encode(f\"|0|{get_length_param(input_user, tokenizer)}|\" \\\n","                                              + input_user + tokenizer.eos_token, return_tensors=\"pt\")\n","        # append the new user input tokens to the chat history\n","        chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n","\n","    if next_who == \"G\":\n","\n","        next_len = input(\"Phrase len? 1/2/3/-\\t\")  #input(\"Exp. len?(-/1/2/3): \")\n","        # encode the new user input, add parameters and return a tensor in Pytorch\n","        new_user_input_ids = tokenizer.encode(f\"|1|{next_len}|\", return_tensors=\"pt\")\n","        # append the new user input tokens to the chat history\n","        chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n","        \n","        #print(tokenizer.decode(chat_history_ids[-1])) # uncomment to see full gpt input\n","        \n","        # save previous len\n","        input_len = chat_history_ids.shape[-1]\n","        # generated a response; PS you can read about the parameters at hf.co/blog/how-to-generate\n","        chat_history_ids = model.generate(\n","            chat_history_ids,\n","            num_return_sequences=1,                     # use for more variants, but have to print [i]\n","            max_length=512,\n","            no_repeat_ngram_size=3,\n","            do_sample=True,\n","            top_k=50,\n","            top_p=0.9,\n","            temperature = 0.6,                          # 0 for greedy\n","            mask_token_id=tokenizer.mask_token_id,\n","            eos_token_id=tokenizer.eos_token_id,\n","            unk_token_id=tokenizer.unk_token_id,\n","            pad_token_id=tokenizer.pad_token_id,\n","            device='cpu'\n","        )\n","        \n","        # pretty print last ouput tokens from bot\n","        print(f\"===> GPT-3:  {tokenizer.decode(chat_history_ids[:, input_len:][0], skip_special_tokens=True)}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"colab":{"name":"g193523(Woman.ru | conversational).ipynb","provenance":[],"collapsed_sections":["d6pGTxL1kg9e"]}},"nbformat":4,"nbformat_minor":5}